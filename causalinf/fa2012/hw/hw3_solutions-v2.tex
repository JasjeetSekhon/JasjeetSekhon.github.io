
\documentclass{article}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{color}
\usepackage{setspace}
\usepackage{fullpage}
\usepackage[round]{natbib}
\usepackage[utf8]{inputenc}
 
\usepackage{fullpage}
\usepackage{boxedminipage}

\usepackage{listings}

\usepackage{minitoc}

\usepackage{ifpdf}

\usepackage{natbib} 
\usepackage{times} 
\usepackage{setspace}
\usepackage{subfigure}

\usepackage{hyperref} 

\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}} 
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}} 
\newcommand{\var}[0]{\text{var}}
\newcommand{\cov}[0]{\text{cov}}
\ifpdf 
\usepackage[pdftex]{graphicx} \else 
\usepackage{graphicx} \fi 

\begin{document}

\title{PS C236A / Stat C239A \\ Problem Set 3 - Solutions}
\date{}
\maketitle

\begin{itemize}
\item[1:] 

First we need to show that $Z \perp X |e(X)$, which is equivalent to
showing $P(Z|X) = P(Z|e(X))$. 
\vspace{1em}

$P(Z|e(X)) = E[Z|e(X)] = E[E[Z|e(x),
x]|e(x)] = E[P(Z|X, e(X))|e(x)]= E[e(x)|e(x)] = e(x) = P(Z|X)$
\vspace{1em}

Now we need to show that conditioning on the propensity score and under the stated assumptions, that the ATT is identified.
We want to estimate $E((r_1-r_0)| Z =1)$  which is the ATT
estimand. Note that conditioning on $Z = 1$ is equivalent to
conditioning on $X|Z = 1$, which--as we proved above--is equivalent to
conditioning on $e(X)|Z = 1$. 
\vspace{1em}

We can observe without assumptions:
$E(r_1|Z=1)-E(r_0|Z=0)$
Because we assume that $r_0 \perp Z,$ then $E(r_0|Z = 0)$ can be rewritten as $E(r_0|Z = 1)$. As a result,
$E(r_1|Z =1)-E(r_0|Z =1)=E(r_1 -r_2|Z =1)$


  \item[2)]
    \begin{itemize}
    \item[a)]
      Note that these $p_i$ we want to estimate are
      propensity scores.
      From Rosenbaum and Rubin, 
      the probability of observing a given treatment assignment is
      $$
        \prod_{i=1}^{10000} e(X_i)^{T_i}(1-e(X_i))^{T_i}
      $$
      where $e(X_i)$ is the propensity score given observed covariates $X_i$.
      From our problem statement, we know that $e(X_i)$ is a function
      of sex, exercising more than 30 minutes a day, 
      and watching TV for more than an hour a day.
      Let $S_i$, $E_i$, and $V_i$ denote indicator variables for these covariates.
      It follows that the probability of our treatment assignment is:
      \begin{eqnarray}
        && \prod_{i=1}^{10000} e(X_i)^{T_i}(1-e(X_i))^{1-T_i} = 
        \prod_{i=1}^{10000} e(S_i,E_i,V_i)^{T_i}(1-e(S_i,E_i,V_i))^{1-T_i}\nonumber \\
        & = & \prod_{(s,e,v) \in \{0,1\}^3} e(s,e,v)^{\#(s,e,v,t)}
              (1-e(s,e,v))^{\#(s,e,v)- \#(s,e,v,t)} \label{eqnsome}
      \end{eqnarray}
      Here, $s,e,$ and $v$, all take values equal to 0 or 1;
       $\#(s,e,v)$ denotes the number of units that have covariate indicators
       $S_i = s$, $E_i = e$, and $V_i = v$; and
      $\#(s,e,v,t)$ denotes the number of treated units that have those values
      of indicator variables.
      
      We take the log of~\eqref{eqnsome} to obtain:
      $$
        \sum_{(s,e,v) \in \{0,1\}^3}  \#(s,e,v,t)\log(e(s,e,v)) +
          (\#(s,e,v)- \#(s,e,v,t))\log
              (1-e(s,e,v))
      $$
      We choose one of the eight possible choices of $(s,e,v)$,
      and denote this choice as $(s^*,e^*,v^*)$
      Taking the derivative with respect to $e(s^*,e^*,v^*)$ 
      setting this equal to zero, and solving for $e(s^*,e^*,v^*)$,
      we obtain:
      \begin{eqnarray*}
       &&\frac{ \#(s^*,e^*,v^*,t)}{e(s^*,e^*,v^*)}- \frac{\#(s^*,e^*,v^*)- \#(s^*,e^*,v^*,t)}{1-e(s^*,e^*,v^*)} = 0 \\
       &\implies & \#(s^*,e^*,v^*,t)(1-e(s^*,e^*,v^*)) = (\#(s^*,e^*,v^*)- \#(s^*,e^*,v^*,t))e(s^*,e^*,v^*)) \\
       &\implies & e(s^*,e^*,v^*) = \frac{\#(s^*,e^*,v^*,t)}{\#(s^*,e^*,v^*)}
      \end{eqnarray*}
      Our estimate of the propensity score $e(s^*,e^*,v^*)$ is
      $$
        \widehat {e(s^*,e^*,v^*)} =  \frac{\#(s^*,e^*,v^*,t)}{\#(s^*,e^*,v^*)},
      $$
      which is the number of treated units having covariates $(s^*,e^*,v^*)$ divided by
      the number of units with covariates $(s^*,e^*,v^*).$
      
      This method of obtaining these
      estimates is called \textit{maximum likelihood estimation}.
    \item[b)]
      By the law of large numbers, the sample proportion of treated units 
      with covariates $(s,e,v)$ will converge to the true proportion of
      treated units with covariates $(s,e,v)$: which is the propensity score $e(s,e,v)$.  
      If, for example, the propensity score does not depend on $V_i$, we will expect
      our estimate of $e(s,e,V_i=1)$ to be the same (asymptotically) as $e(s,e,V_i= 0)$; 
      this convergence is not affected if only a subset of these covariates affect the
      propensity score.
      
      (Note: Under the assumption that the propensity score only depends on a subset of
      the three covariates, this method of finding a balance
      score is finer than the true propensity score.  
      Conditioning on this balance score will give you the same estimates (asymptotically)
      as conditioning on the propensity score.)
    \item[c)]
      Let $C_i(s,e,v) = 1$ if unit $i$ has $S_i = s$, $E_i = e$ and $V_i = v$.
      The standard estimate for the ATE is
      $$
        \frac 1 N\sum_{(s,e,v) \in \{0,1\}^3}\sum_{i = 1}^{10000} C_{i}(s,e,v)\left(
          \frac{Y_i(1)T_i}{e(s,e,v)} - \frac{Y_i(0)(1-T_i)}{1- e(s,e,v)}
        \right).
      $$
      A possible estimate for the ATT is
      $$
       \sum_{(s,e,v) \in \{0,1\}^3}\sum_{i = 1}^{10000} \frac{\#(s,e,v,t)}{\sum T_i}\frac{C_{i}(s,e,v)}{\#(s,e,v)}\left(
          \frac{Y_i(1)T_i}{e(s,e,v)} - \frac{Y_i(0)(1-T_i)}{1- e(s,e,v)}
        \right).
      $$
      We see that the estimate of the ATE is unbiased:
      \begin{eqnarray*}
       && \E\left(\frac 1 N\sum_{(s,e,v) \in \{0,1\}^3}\sum_{i = 1}^{10000} C_{i}(s,e,v)\left(
          \frac{Y_i(1)T_i}{e(s,e,v)} - \frac{Y_i(0)(1-T_i)}{1- e(s,e,v)}
        \right)\right)\\
       &=&\frac 1 N\sum_{(s,e,v) \in \{0,1\}^3}\sum_{i = 1}^{10000} C_{i}(s,e,v)\left(
          \frac{Y_i(1)\E(T_i)}{e(s,e,v)} - \frac{Y_i(0)(1-\E(T_i))}{1- e(s,e,v)}
        \right)\\
      &=&\frac 1 N\sum_{(s,e,v) \in \{0,1\}^3}\sum_{i = 1}^{10000} C_{i}(s,e,v)\left(
          \frac{Y_i(1)e(s,e,v)}{e(s,e,v)} - \frac{Y_i(0)(1-e(s,e,v))}{1- e(s,e,v)}
        \right)\\
      &=&\frac 1 N\sum_{(s,e,v) \in \{0,1\}^3}\sum_{i = 1}^{10000} C_{i}(s,e,v)\left(
          Y_i(1) - Y_i(0)
        \right)\\
      & = &\frac 1 N\sum_{i = 1}^{10000}
          Y_i(1) - Y_i(0) = ATE\\
      \end{eqnarray*}
      %CHECK THIS BEFORE YOU WRECK THIS
      The estimate of the ATT is unbiased as well, though it takes a bit more work to show:
      \begin{eqnarray*}
       &&\E\left(\sum_{(s,e,v) \in \{0,1\}^3}\sum_{i = 1}^{10000}
       \frac{\#(s,e,v,t)}{\sum T_i}\frac{C_{i}(s,e,v)}{\#(s,e,v)}
       \left(
          \frac{Y_i(1)T_i}{e(s,e,v)} - \frac{Y_i(0)(1-T_i)}{1- e(s,e,v)}
        \right)\right)\\
      &=&\sum_{(s,e,v) \in \{0,1\}^3}\sum_{i = 1}^{10000} \frac{\#(s,e,v,t)}{\sum T_i}\frac{C_{i}(s,e,v)}{\#(s,e,v)}\left(
          \frac{Y_i(1)\E(T_i)}{e(s,e,v)} - \frac{Y_i(0)(1-\E(T_i))}{1- e(s,e,v)}
        \right)\\
      & = &\sum_{(s,e,v) \in \{0,1\}^3}\sum_{i = 1}^{10000} \frac{\#(s,e,v,t)}{\sum T_i}C_{i}(s,e,v)\left(
          \frac{Y_i(1)}{\#(s,e,v)} - \frac{Y_i(0)}{\#(s,e,v)}
        \right)\\
      & = &\sum_{(s,e,v) \in \{0,1\}^3} \frac{\#(s,e,v,t)}{\sum T_i}\E\left(Y_i(1) - Y_i(0) | (s,e,v)\right)
        \\
      & = & E_{X_{T = 1}}[E(Y_i(1) | (s,e,v))] - E_{X_{T = 1}}[E(Y_i(0) | (s,e,v))] 
      \end{eqnarray*}
     Now, under the assumption $Y_i \independent T_i | X_i$,
     \begin{eqnarray*}
      && E_{X_{T = 1}}[E(Y_i(1) | (s,e,v))] - E_{X_{T = 1}}[E(Y_i(0) | (s,e,v))] \\
      &=&E_{X_{T = 1}}[E(Y_i(1) | T_i = 1, (s,e,v))] - E_{X_{T = 1}}[E(Y_i(0) | T_i = 1, (s,e,v))] \\
      &=& E(Y_i(1) - Y_i(0) | T_i = 1)
     \end{eqnarray*}
     If the propensity score is unknown, we may estimate it as in part a).
     \item[d)]  The ATE cannot be estimated by conditioning on the propensity score: 
       the common support assumption is violated (though an ATE for people not weighing 500
       pounds could be estimated).
       Specifically, people that weigh 500 pounds will never be treated.
       However, the assumptions necessary to estimate the ATT do hold.
       
       The ATT could be estimated a few different ways. 
       Probably the most common method to estimate is to
       match treated people to control people using the propensity score in some way
       (for example, by matching to the nearest neighbor with replacement), 
       and then
       to take the average of the difference between 
       the treated units and the matched control units.
     \item[e)] We are assuming the model:
     $$
       T_i = \alpha + \beta\text{weight}_i + \epsilon_i
     $$
     where $\epsilon_i$ has the distribution
     $$
       \epsilon_i = \left\{ 
         \begin{array}{ll}
           1-\alpha - \beta\text{weight}_i) & \text{with probability }=\alpha + \beta\text{weight}_i,\\
           -\alpha - \beta\text{weight}_i & \text{with probability } = 1 - \alpha -\beta\text{weight}_i.
         \end{array}
       \right.
     $$
     Note that:
     \begin{eqnarray*}
       \E(\epsilon_i) &=&  (1-\alpha - \beta\text{weight}_i))(\alpha + \beta\text{weight}_i)
       + ( -\alpha - \beta\text{weight}_i )(1 - \alpha -\beta\text{weight}_i) \\
       &=& (1-\alpha - \beta\text{weight}_i))(\alpha + \beta\text{weight}_i)
       - ( \alpha + \beta\text{weight}_i )(1 - \alpha -\beta\text{weight}_i) = 0
     \end{eqnarray*}
     Thus, letting 
     $$
       X = \left(
        \begin{array}{cc}
          1 & \text{weight}_1 \\
          1 & \text{weight}_2 \\
          \vdots & \vdots \\
          1 & \text{weight}_N
        \end{array}
      \right)
      \text{ and }
      \epsilon = \left(
        \begin{array}{c}
         \epsilon_1 \\
         \epsilon_2\\
         \vdots \\
         \epsilon_N
        \end{array}
      \right)
     $$ 
     the expectation of the OLS estimate is
     \begin{eqnarray*}
       \E[(X'X)^{-1}X'Y] &=& \E[(X'X)^{-1}X'(X(\alpha, \beta)') + \epsilon)]     \\
       &=& \E[(X'X)^{-1}X'(X(\alpha, \beta)')] +\E[(X'X)^{-1}X'\epsilon] \\
       & = & (\alpha,\beta)' + (0,0)' = (\alpha,\beta)'  
     \end{eqnarray*}
     Thus, even though the $\epsilon$ are not identically distributed, 
     because they all have $\E(\epsilon_i) = 0$, OLS will still 
     estimate the $\alpha$ and $\beta$ parameters unbiasedly.
  \end{itemize}
  \item[3)]
  \begin{itemize}
    \item[a)]  This question is better worded as:
      ``Show that, when estimating a regression within a bounded region,
        the estimates at the boundaries are more variable than anywhere else.''
       
       Suppose the regression is fitted through $N$ points in total.
       Assume that errors for the regression are i.i.d with mean 0 and
       variance $\sigma^2$.
       It can be shown 
       (for example, in Mathematical Statistics and Data Analysis by John Rice) 
       that the variance for the regression estimate at $x_i$ is
       $$
         \sigma^2\left(\frac 1 N + \frac {(x_i - \bar x)^2}{\sum_{j=1}^N (x_j - \bar x)^2}\right)
       $$
       This variance increases as the distance between $x_i$ and $\bar x$ grows;
       estimates are most variable at the boundary.
       
       Estimates of the LATE can compare the regression estimates at the cut points, 
       which can be quite variable if the cut point is far away from the mean of the 
       regressed covariate.
     \item[b,c)] In b) and c), I meant to write that the coin flip only reduced scores for
       students scoring between $c$ and $c + 10$ points.  
       In which case, the smoothness of covariates at the cut point is violated:
       Below the cut point contains people who scored between
       $c-5$ and $c$, as well as those that scored $c+5$ and $c+10$ points on
       the exam.  
       Above the cut point contains future income only include people that scored 
       between $c$ and $c+5$ points.
       Asymptotically, you get imbalance in the Test Score Before Coin Toss covariate 
       at the cut point.
       
       However, when regressing on data to the left of the cut point, 
       the regression estimate for the future outcome for at the cut point will be higher
       when this coin flip mechanism is in place than when it is not in place:
       the estimate of the LATE (the difference of the regression estimates at the cutpoint)
       will be smaller with the coin flip than without the coin flip.
       Without the coin flip, this estimate be unbiased for the LATE; 
       so with the coin flip, this estimate will be a lower bound (asymtotically) to 
       the true LATE.
       
       Instead, suppose additionally that the distribution of scores on the interval
       $(c, c+5)$ is the same of the distribution of the scores on the interval $(c+10,c+15)$,
       which is closer to the interpretation of the problem as written.
       (By distribution of the scores is the same, I mean that: if you graph a histogram
       of the test scores within $(c,c+5)$, and another histogram 
       of the test scores within $(c+10,c+15)$,
       the histograms would look the same.)
       We assume the model written in part c).

       We introduce some notation:
       Let $x_i$ denote the pre-coin-flip test score of person $i$.
       Let $z_i$ be defined as
       $$
         z_i = \left \{
           \begin{array}{ll}
             y_i, & x_i < c, \\
             y_i - 10\beta, & x_i \geq c.
           \end{array}
         \right.
       $$
              
       Suppose that, of all people with test scores after the coin flip 
       within $(c - 5, c)$, $N^-$ of those people
       had test scores before the coin flip within $(c-5, c)$,
       and $N^+$ had test scores within $(c+5, c+15)$.
       Similarly, suppose that, of all people with post-coin-flip test scores
       within $(c,c+5)$, $M^-$ people had pre-coin-flip test scores between
       $(c,c+5)$ and $M^+$ had test scores between $(c+10,c+15)$.
       Suppose that people with post-coin-flip scores between $(c-5,c)$
       are ordered so that the first $N^-$ people correspond to those having
       pre-test-scores within $(c,c+5)$, and the last $N^+$ people have
       scores within $(c+10,c+15)$.
       
       By the equal in distribution assumption,
       after the coin flip, the mean and the variance of the test scores within 
       $(c, c+5)$ will be the same as before the coin flip.
       The mean future incomes will now have an average (asymptotically)
       of
       $$
         \frac{N^-(\bar z) + N^+(\bar z + 10\beta)}{N^- + N^+} 
         = \bar z + \frac{N^+}{N^-+N^+}10\beta.
       $$
       Also by the equal distribution assumption, asymptotically we have
       \begin{eqnarray*}
          \sum_{i=1}^{N^-} (x_i - \bar x)^2
            & =& \frac{N^-}{N^- + N^+}\sum_{i=1}^{N^-+N^+} (x_i -\bar x)^2\\
          \sum_{i=1}^{N^-} (x_i - \bar x)(z_i - \bar z) 
            &=&  \frac{N^-}{N^- + N^+}\sum_{i=1}^{N^-+N^+}(x_i - \bar x)(z_i - \bar z) \\
          \sum_{i=N^++1}^{N^- + N^+} (x_i - \bar x) 
            & = & \frac{N^+}{N^- + N^+}\sum_{i=1}^{N^-+N^+}(x_i - \bar x)
       \end{eqnarray*}
       And we have
       \begin{eqnarray}
         && \sum_{i=1}^{N^-} (x_i - \bar x)(z_i - \bar z - \frac{N^+}{N^-+N^+}10\beta ) +
              \sum_{i=N^-+1}^{N^-+N^+} (x_i - \bar x)(z_i + 10\beta - \bar z - \frac{N^+}{N^-+N^+}10\beta)
              \nonumber \\
              &=& \sum_{i=1}^{N^-+N^+}(x_i -\bar x)(z_i - \bar z) - 
              \frac{N^+}{N^-+N^+}10\beta\sum_{i=1}^{N^-}(x_i - \bar x) \nonumber \\
              &&+ 10\beta\sum_{i = N^-+1}^{N^-+N^+}(x_i - \bar x)
              - \frac{N^+}{N^-+N^+} 10\beta \sum_{i = N^-+1}^{N^-+N^+} (x_i - \bar x) \nonumber \\
              &=&\sum_{i=1}^{N^-+N^+}(x_i -\bar x)(z_i - \bar z) 
              - \frac{N^+}{N^-+N^+} 10\beta \sum_{i = 1}^{N^-+N^+} (x_i - \bar x)
              + 10\beta\sum_{i = N^-+1}^{N^-+N^+}(x_i - \bar x) \nonumber \\
              &\approx& \sum_{i=1}^{N^-+N^+}(x_i -\bar x)(z_i - \bar z)
               - 10\beta\sum_{i = N^-+1}^{N^-+N^+}(x_i - \bar x)+ 
               10\beta\sum_{i = N^-+1}^{N^-+N^+}(x_i - \bar x) \nonumber\\
              & = &  \sum_{i=1}^{N^-+N^+}(x_i -\bar x)(z_i - \bar z) \label{hereseqn}
       \end{eqnarray}
       
       Consider fitting a regression to people with test scores 
       within $(c-5,c)$.
       The slope of the regression line pre-coin-flip will be
       \begin{eqnarray*}
         \hat\beta& =&\frac{\sum_{i=1}^{N^-} (x_i - \bar x)(z_i - \bar z) }{ \sum_{i=1}^{N^-} (x_i - \bar x)^2}\\
            &\approx&\frac{ \frac{N^-}{N^- + N^+}\sum_{i=1}^{N^-+N^+} (x_i - \bar x)(z_i - \bar z) }
            { \frac{N^-}{N^- + N^+}\sum_{i=1}^{N^- + N^+} (x_i - \bar x)^2} \\
         &=& \frac{\sum_{i=1}^{N^-+N^+} (x_i - \bar x)(z_i - \bar z)}{\sum_{i=1}^{N^- + N^+} (x_i - \bar x)^2}
       \end{eqnarray*}
       And, by~\eqref{hereseqn}, the regression line after the coin flip will be:
       \begin{eqnarray*}
         \hat\beta&=&\frac{
           \sum_{i=1}^{N^-} (x_i - \bar x)(z_i - \bar z - \frac{N^+}{N^-+N^+}10\beta ) +
              \sum_{i=N^-+1}^{N^-+N^+} (x_i - \bar x)(z_i + 10\beta - \bar z - \frac{N^+}{N^-+N^+}10\beta}
             {\sum_{i=1}^{N^- + N^+} (x_i - \bar x)^2}\\
            &\approx&
            \frac{\sum_{i=1}^{N^-+N^+} (x_i - \bar x)(z_i - \bar z)}{\sum_{i=1}^{N^- + N^+} (x_i - \bar x)^2}
       \end{eqnarray*}
       Thus, asymptotically, the slope of the fitted regression lines before and after the coin flip
       will be the same.
       
       Additionally, since the new estimated intercept of the regression line is
       \begin{eqnarray*}
          \bar y - \hat\beta \bar x = 
          \bar z + \frac{N^+}{N^-+N^+}10\beta - \hat\beta\bar x
       \end{eqnarray*}
       it follows that the post-coin-flip estimate at the cutpoint is
       $$
         \hat y_{post} = \hat y_{pre}+ \frac{N^+}{N^-+N^+}10\beta 
       $$ 
       
       Following the same process, we find that
       a similar result holds for the regression fitted to people 
       with post-coin-flip test scores
       in $(c,c+5)$:
       $$
         \hat y_{post} = \hat y_{pre}+ \frac{M^+}{M^-+M^+}10\beta 
       $$
       It follows that, if $\frac{N^+}{N^-+N^+} =  \frac{M^+}{M^-+M^+}$
       the estimate of the LATE will be unbiased when regressing on the
       post-coin-flip test scores.
     \end{itemize}
\end{itemize}     

\item[4.]


\end{itemize}
\end{document}




  \item[2:]  
  \begin{itemize}
  \item[a)]
    There are $2^n$ possible ways that treatment can be assigned to the $n$ units.
    Each unit $i$ can be assigned either treatment $T_i = 1$ or $T_i = 0$.
    There is no restriction in the problem description on the number of units that can be assigned treatment.
    Since there are $n$ units in total, there are $n2^n$ total potential outcomes in this experiment.
  \item[b)]
    We break this down into two cases.\\
    \textbf{Case 1:} $\sum_{i=1}^n T_i < n/2$.  
    The outcome when unit $i$ is treated is the same for all treatment assignments satisfying this case with $T_i = 1$.
    The outcome when unit $i$ is not treated is the same for all treatment assignments satisfying this case with $T_i = 0$.
    Thus, there are two outcomes for each unit when treatment assignment satisfies the above condition,
    the one when the unit is assigned to treatment and the one when assigned to control.\\
     \textbf{Case 2:} $\sum_{i=1}^n T_i \geq n/2$.  
    When this happens, each treatment assignment has its own potential outcome. \\
    It follows that the number of potential outcomes is
    \begin{eqnarray*}
      \#\text{outcomes} &=& \#\text{outcomes when }  \sum_{i=1}^n T_i < n/2 +  \#\text{outcomes when } \sum_{i=1}^n T_i \geq n/2\\
     & = & 2 + \sum_{i = \lceil n/2 \rceil}\#\text{ways exactly $i$ treatments can be assigned}  \\
     &= & 2 + \sum_{i = \lceil n/2 \rceil}^n \binom{n}{i}\\
     & = & \left\{
       \begin{array}{ll}
         2 + 2^{n-1} & n \text{ is odd}\\
         2 + 2^{n-1} + \frac 1 2\binom{2n}{n} & n \text{ is even}
       \end{array}
     \right.
    \end{eqnarray*}
    where $\lceil x \rceil$ denotes ``round x up to the nearest whole number.''\\
    These formulas can be derived by the properties:     
    \begin{align*}
       \sum_{i = 0}^n \binom n i &= 2^n \\ \intertext{and}
      \sum_{i = 0}^k \binom n i &= \sum_{i = n-k}^n \binom n i.
    \end{align*}
  \item[c)]
    Note, since unit 1 and unit $n$ only have one neighbor, 
    both of these units only have two potential outcomes.
    Now, consider unit $i$ where $1 < i < n$.
    Unit $i$'s response depends on whether $i +1$ and $i-1$ both get treatment.
    However, unit $i+1$'s response is affected by whether units $i$ and $i+2$ get treatment;
    the outcome of $i$ when units $i-1$, $i$, $i+1$ and $i+2$ all get treatment may be different from 
    the outcome where units $i-1$, $i$, $i+1$ get treatment, but $i+2$ gets control.
    Thus, we obtain the following cases:\\
    \textbf{Case 1:}
      Units $i-1$ and units $i + 1$ are not both treated.\\
      In this case, unit $i$ has two potential outcomes, 
      response under treated and response under control.\\
    \textbf{Case 2:}
      Units $i-1$ and units $i + 1$ are both treated, 
      but unit $i$ is not treated. \\
      In this case, units $i-1$ and units $i+1$'s response are not affected 
      by neighboring units, and so, there is only one outcome.\\
    \textbf{Case 3:} 
      Units $i-1$, $i$ and $i+1$ are all treated.\\
      In this case, the response of $i$ is also going 
      to depend on whether $i-2$ is treated 
      (as if $i-2$ is treated, then $i-1$ will have interference, otherwise it will not). 
      Likewise, it will also depend on whether $i+2$ is treated.
      Moreover, if $i-2$ is also treated, the response of $i$ 
      will also depend on whether or not $i-3$ is treated, and so on.
      It follows that there are $i-1$ cases to consider to the left of unit $i$,
      and $n-(i+1)$ cases to consider to the right of unit $i$, leading to 
      $(i-1)(n-(i+1))$ potential outcomes.\\
    Thus, in total, there are 
    $$
      2+1+(i-1)(n-(i+1))
    $$ 
    potential outcomes.
  \end{itemize}
  \item[3)]
  \begin{itemize}
    \item[a)] The average treatment effect parameter is
      $$
        \bar \tau = \frac 1 N \sum_{i=1}^N x_i - \frac 1 N \sum_{i=1}^N y_i.
      $$
      That is, $\bar \tau$ is the difference of the average effect of treatment $A$ 
      over all $N$ subjects and the average effect of treatment $B$ over all $N$ subjects.  
    \item[b)]
      \begin{align*}
	&\var(\bar X - \bar Y) = \var(\bar X) + \var(\bar Y) - 2\cov(\bar X,\bar Y) \\
	=& \frac 1 {N-1}\left( {(N - n)}\frac{ \sigma^2}n + {(N-m)}\frac{\tau^2}m + 2\cov(x,y)\right)\\
	= &\frac N {N-1}\left(\frac{\sigma^2}{n}+ \frac{\tau^2}{m}\right) - 
	\frac{1}{N-1}(\sigma^2 + \tau^2 - 2\cov(x,y)).
       \end{align*}
   \item[c)]
     From part b),
       $$
	\frac N {N-1}\left(\frac{\sigma^2}{n}+ \frac{\tau^2}{m}\right) - 
	\var(\bar X - \bar Y) = \frac{1}{N-1}(\sigma^2 + \tau^2 - 2\cov(x,y)).
       $$
     The right-hand side is the difference between our formula and the usual formula.  
     This quantity is not identifiable as the covariance of $x$ and $y$ is never observed.
   \item[d)]
     Note that,
       $$
         \sigma^2 + \tau^2 - 2\cov(x,y) = \var(x) + \var(y) - 2\cov(x,y) = \var(x-y) \geq 0.
       $$
     Thus, the ``usual'' estimate greater than or equal to the truth asymptotically.  
     The bias will be 0 when $\var(x-y) = 0$.  
     This is only true if $x_i = y_i$ for $1\leq i \leq N$.  
     That is, this is only true if the sharp null of no treatment effect of $B$ relative to $A$
     for all subjects $i$ in the population is true.
  \end{itemize}
  \item[4)]
  \begin{itemize}
    \item[a)]The design matrix for the correct model is
      $$
        \left(
          \begin{array}{ccc} 
            1 & \text{education level}_1 & \text{intelligence}_1 \\
            1 & \text{education level}_2 & \text{intelligence}_2 \\
            \vdots & \vdots & \vdots \\
            1 & \text{education level}_N & \text{intelligence}_N 
          \end{array}
        \right)
      $$
      and for the incorrect model is
      $$
        \left (
          \begin{array}{cc}
            1 & \text{education level}_1  \\
            1 & \text{education level}_2  \\
            \vdots & \vdots  \\
            1 & \text{education level}_N 
          \end{array}
        \right ).
      $$
    \item[b)]
      \label{partb}
      It is equivalent to show
      $$
        \sum_{i=1}^N (y_i - \hat y_i) = 0.
      $$
      Consider 
      $$
        Y - \hat Y = Y - X(X'X)^{-1}X'Y = (1 - X(X'X)^{-1}X)Y.
      $$
      Note that 
      \begin{align*}
        X'(Y - \hat Y) = & X'(1 - X(X'X)^{-1}X')Y = (X' - X'X(X'X)^{-1}X')Y \\
        = & (X' - X')Y = \mathbf 0.
      \end{align*}
      Since $X$ has a column of 1's, then we must have that
      \begin{equation*}
        (1, 1, \ldots, 1)(Y - \hat Y) = \sum_{i=1}^N (y_i - \hat y_i) = 0.
      \end{equation*}
    \item[c)]
      Part b) is guaranteed to be true by the properties of OLS.
      All that is necessary is that the design matrix is full rank 
      and that the model has an intercept term (hence, the design matrix has a column of ones).
    \item[d,e)]
      Although it's not explicitly stated, we assume
      $$
        E(\epsilon_{i1}| \text{correct design matrix}) = 0.
      $$
      By running the naive model instead of the true model, 
      you are introducing ``omitted variable bias'' into your 
      estimate of the effect of education. 
      The proof is as follows: \\[1ex]
      Let $X$ denote the incorrect model, 
      let $Z = (\text{intelligence}_1,  \text{intelligence}_2,\ldots, \text{intelligence}_N)'$,
      let $\beta_0 = (\alpha_2,\beta)'$, and let $\epsilon_1 = (\epsilon_{11},\epsilon_{12}, \ldots, \epsilon_{1n})$.
      The expectation of the OLS estimates for the coefficients in the second model 
      (assuming that the values of the covariates is fixed) is 
      \begin{eqnarray*}
        E(\hat \beta_0|X,Z) &=& E((X'X)^{-1}X'Y) = E((X'X)^{-1}X'(X\beta_0 + Z\gamma_2 + \epsilon_{1})|X,Z)\\
        &=& E((X'X)^{-1}(X'X)\beta_0 + (X'X)^{-1}(X'Z)\gamma_2 + (X'X)^{-1}X'\epsilon_{1}|X,Z)\\
        & = & E(\beta_0 +  (X'X)^{-1}(X'Z)\gamma_2 + (X'X)^{-1}X'\epsilon_{1}|X,Z) \\
        & = & \beta_0 + (X'X)^{-1}(X'Z)\gamma_2 + 0 \neq \beta_0.
      \end{eqnarray*}
      In particular, our parameter $\beta$ is not estimated 
      unbiasedly unless the second row of the matrix\\
      $(X'X)^{-1}(X'Z) = 0$.  
      We now try to identify exactly when this happens. \\
      Let $\ell_i$ denote $\text{education level}_i$ and let
      $g_i$ denote $\text{intelligence}_i$.
      We find that 
      $$
        (X'X) = \left(
          \begin{array}{cc}
            n & \sum \ell_i\\
            \sum \ell_i & \sum \ell_i^2
          \end{array}
        \right)
      $$
      and so
      $$
        (X'X)^{-1} = \frac{1}{n\sum \ell_i^2- (\sum \ell_i)^2}
        \left(
          \begin{array}{cc}
            \sum \ell_i^2 & - \sum \ell_i \\
            -\sum \ell_i & n
          \end{array}
        \right).
      $$
      It follows that the bottom row of the matrix $(X'X)^{-1}(X'Z)$ is proportional to
      $$
        (-\sum \ell_i, n)(\sum g_i,\sum \ell_ig_i)' = n\sum \ell_ig_i - \sum \ell_i\sum g_i.
      $$
      It is easy to show that this is equation zero if and only if the correlation between 
      $(\ell_i)_{i=1}^n$ and $(g_i)_{i=1}^n$ is zero.\\[1ex]
      Since intelligence and education are positively correlated, and since the estimate of $\beta$ is unbiased
      if and only if intelligence and education are uncorrelated
      it follows that our estimate of $\beta$ is biased.      
      Thus, our estimate of $\beta$ is not BLUE, since it is not unbiased.
    \item[f)]  Note that:
      \begin{eqnarray*}
        \hat \beta_0 - E(\hat \beta_0) &=& (X'X)^{-1}X'Y - (\beta_0 + (X'X)^{-1}(X'Z)\gamma_2) \\
        &=& (X'X)^{-1}X'(X\beta_0 + Z\gamma_2 + \epsilon_{1}) - \beta_0 -(X'X)^{-1}(X'Z)\gamma_2\\
        &=&\beta_0 +(X'X)^{-1}X'Z\gamma_2 + (X'X)^{-1}X'\epsilon_1 - \beta_0 - (X'X)^{-1}(X'Z)\gamma_2\\
        &=& (X'X)^{-1}X'\epsilon_1.
      \end{eqnarray*}
      It follows that:
      \begin{eqnarray*}
        \cov(\hat \beta_0|X,Z) &=& E[(\hat \beta_0 - E(\hat \beta_0))(\hat \beta_0 - E(\hat \beta_0))'|X,Z]\\
        &=&E[((X'X)^{-1}X'\epsilon_1)((X'X)^{-1}X'\epsilon_1)'|X,Z] \\
        & = &E[((X'X)^{-1}X'\epsilon_1\epsilon_1'X((X'X)^{-1})')|X,Z]\\
        & = & (X'X)^{-1}X'E[\epsilon_1\epsilon_1'|X,Z]'X((X'X)^{-1})' \\
        & = & (X'X)^{-1}X'I_{n\;x\;n}\sigma_1^2X((X'X)^{-1})' \\
        & = & \sigma_1^2 (X'X)^{-1}X'I_{n\;x\;n}X((X'X)^{-1})'\\
        & = & \sigma_1^2  (X'X)^{-1}X'X((X'X)^{-1})' \\
        &= & \sigma_1^2(X'X)^{-1}.
        %\cov((X'X)^{-1}X'Y) = \cov((X'X)^{-1}X'(X\beta + Z\gamma_2 + \epsilon_{1})|X,Z)\\
        %&=& \cov((X'X)^{-1}(X'X)\beta_0 + (X'X)^{-1}(X'Z)\gamma_2 + (X'X)^{-1}X'\epsilon_{1}|X,Z)\\
        %& = & \cov(\beta_0 +  (X'X)^{-1}(X'Z)\gamma_2 + (X'X)^{-1}X'\epsilon_{1}|X,Z) \\
       % & = & 0 + 0 + (X'X)^{-1}X'\sigma_1^2((X'X)^{-1}X')' = \sigma^2(X'X)^{-1}(X'X)(X'X)^{-1} =(X'X)^{-1}\sigma_1^2.
      \end{eqnarray*}
      The last equality uses the fact that $(X'X)^{-1}$ is symmetric.
      The covariance of our estimate for $\beta$ is just simply the 
      entry in the second row and second column of $(X'X)^{-1}\sigma_1^2.$
      From part d), it follows that the covariance of our estimate is
      $$
        \sigma^2\frac{n}{n\sum \ell_i^2 - (\sum \ell_i)^2} = 
        \sigma^2\frac{1}{\sum \ell_i^2 - n(\bar \ell)^2} = \sigma^2\frac{1}{\sum (\ell_i - \bar\ell)^2}.
      $$
  \end{itemize}
  \item[5)]
  
    We will prove these results in a similar way as in problem 4.
    Let $X$ denote the design matrix:
    $$
      X = \left(
        \begin{array}{cc} 
          1 & \text{T}_1\\
          1 & \text{T}_2  \\
          \vdots & \vdots \\
          1 & \text{T}_N    
        \end{array}
      \right)
    $$
    Let $Z_a$ denote the design matrix for the model in part a).
    $$
      Z_a = \left(
        \begin{array}{ccc} 
          1 & \text{T}_1 & S_{t-1,1}\\
          1 & \text{T}_2  & S_{t-1,2}\\
          \vdots & \vdots & \vdots \\
          1 & \text{T}_N    & S_{t-1,N}
        \end{array}
      \right)
    $$
    Let $Z_b$ denote the design matrix for the model in part b).
    $$
      Z_b = \left(
        \begin{array}{cccc} 
          1 & \text{T}_1 & S_{t-1,1} & S_{t+1,1}\\
          1 & \text{T}_2  & S_{t-1,2} & S_{t+1,2}\\
          \vdots & \vdots & \vdots & \vdots\\
          1 & \text{T}_N    & S_{t-1,N}& S_{t+1,N}
        \end{array}
      \right)
    $$
    Note, if we estimate the model
    \begin{equation}
      S_{t+2,i} = \alpha + T_i\beta_1 + \epsilon_i
      \label{singlemodel}
    \end{equation}
    using OLS, we find that our estimate for $\beta_1$ is
    $$
     \frac{\sum(T_i - \overline T)(S_{t+2,i} - \overline {S_{t+2}})}{\sum(T_i - \overline T)^2} =\left(\frac{1}{\sum T_i} \sum_{i : T_i = 1} S_{t+2,i}\right) - \left(\frac{1}{N- \sum T_i} \sum_{i: T_i = 0} S_{t+2,i}\right)
    $$
    which is an unbiased estimate (asymptotically) for the ATE.
    \begin{itemize}
    \item[a)] 
      To show that the estimate for $\beta_1$ is unbiased, 
      we will show that the estimate of this model is the same (asymptotically)
      as the estimate for $\beta_1$ in~\ref{singlemodel}. 
      Are argument will be entirely brute-forced; there are far more elegant ways 
      to prove this result.\\[1ex]
      Let $\beta_0 = (\alpha,\beta_1,\beta_2)'$.
      The OLS estimate for $\beta_0$ is 
      $$
        (Z_a'Z_a)^{-1}Z_a'S_{t+2}
      $$
      We are interested in the second row of this estimate.  
      Now (thanks to the magic of Wolfram Alpha), the second row of $(Z_a'Z_a)^{-1}$ is
      $$
        C\left(
          \left(\sum T_i \sum S_{t-1,i}^2 - \sum S_{t-1,i}\sum S_{t-1,i}T_i\right),
          \left((\sum S_{t-1,i})^2-n\sum S_{t-1,i}^2\right),
          \left(n\sum S_{t-1,i}T_i-\sum T_i\sum S_{t-1,i}\right)
        \right)
      $$
      where
      $$
        C = \frac 1 {-n\sum{T_i^2}\sum S_{t-1,i}^2 + n(\sum T_iS_{t-1,i})^2 + 
        (\sum{T_i})^2\sum S_{t-1,i}^2- 2\sum T_i\sum S_{t-1,i}\sum T_iS_{t-1,i}
        + (\sum S_{t-1,i})^2\sum T_i^2}
      $$
      Also,
      $$
        Z_a'S_{t+2} = \left(
          \sum S_{t+2,i}, \sum T_iS_{t+2,i}, \sum S_{t-1,i}S_{t+2,i}
        \right)'
      $$
      It follows that the second row of the OLS estimate is proportional to
      \begin{eqnarray*}
        && \sum S_{t+2,i}\sum T_i \sum S_{t-1,i}^2 - \sum S_{t+2,i}\sum S_{t-1,i}\sum S_{t-1,i}T_i\\
        &+&\sum T_iS_{t+2,i}(\sum S_{t-1,i})^2 - n\sum T_iS_{t+2,i}\sum S_{t-1,i}^2\\
        &+&n\sum S_{t-1,i}S_{t+2,i}\sum S_{t-1,i}T_i-\sum S_{t-1,i}S_{t+2,i}\sum T_i\sum S_{t-1,i}
      \end{eqnarray*}
      Now, treatment should be (asymptotically) uncorrelated with $S_{t-1,i}$, since treatment
      is assigned independently of previous test scores.
      Thus, the constant $C$ simplifies to
      \begin{eqnarray*}
        C & =&\frac 1 {-n\sum{T_i^2}\sum S_{t-1,i}^2 + n(\sum T_iS_{t-1,i})^2 + 
        (\sum{T_i})^2\sum S_{t-1,i}^2- 2\sum T_i\sum S_{t-1,i}\sum T_iS_{t-1,i}
        + (\sum S_{t-1,i})^2\sum T_i^2}\\
        &=& \frac 1 {
        \begin{array}{ll} & \sum S_{t-1,i}^2((\sum{T_i})^2-n\sum{T_i^2})+ 
        \sum T_iS_{t-1,i}(n(\sum T_iS_{t-1,i}) - \sum T_i\sum S_{t-1,i}) \\
        - & \sum T_i\sum S_{t-1,i}\sum T_iS_{t-1,i}
        + (\sum S_{t-1,i})^2\sum T_i^2
        \end{array}} \\
        &= & \frac 1 {-n\sum S_{t-1,i}^2\sum (T_i - \overline T)^2 
          + \sum T_iS_{t-1,i}(0) - \sum S_{t-1,i}(\sum T_i\sum T_iS_{t-1,i}
        - \sum S_{t-1,i}\sum T_i^2)} \\
        & = & \frac 1 {-n\sum S_{t-1,i}^2\sum (T_i - \overline T)^2  - \sum S_{t-1,i}(\sum T_i\sum T_iS_{t-1,i}
        - \sum S_{t-1,i}\sum T_i^2)}
      \end{eqnarray*}
      and we can simplify
       \begin{eqnarray*}
        && \sum S_{t+2,i}\sum T_i \sum S_{t-1,i}^2 - \sum S_{t+2,i}\sum S_{t-1,i}\sum S_{t-1,i}T_i\\
        &&+\sum T_iS_{t+2,i}(\sum S_{t-1,i})^2 - n\sum T_iS_{t+2,i}\sum S_{t-1,i}^2\\
        &&+n\sum S_{t-1,i}S_{t+2,i}\sum S_{t-1,i}T_i-\sum S_{t-1,i}S_{t+2,i}\sum T_i\sum S_{t-1,i}\\
        &=&- \sum S_{t-1,i}^2( n\sum T_iS_{t+2,i}-\sum S_{t+2,i}\sum T_i)  \\
        &&-\sum S_{t-1,i}( \sum S_{t+2,i}\sum S_{t-1,i}T_i -\sum T_iS_{t+2,i}\sum S_{t-1,i})\\
        &&+\sum S_{t-1,i}S_{t+2,i}(n\sum S_{t-1,i}T_i-\sum T_i\sum S_{t-1,i})\\
        & = &-n\sum S_{t-1,i}^2\sum (T_i - \overline T)(S_{t+2,i} - \overline{S_{t+2}})  \\
        &&-\sum S_{t-1,i}( \sum S_{t+2,i}\sum S_{t-1,i}T_i -\sum T_iS_{t+2,i}\sum S_{t-1,i})
      \end{eqnarray*}
    Moreover,
    \begin{eqnarray*}
        & &n\sum S_{t-1,i}^2\sum (T_i - \overline T)^2\sum S_{t-1,i}( \sum S_{t+2,i}\sum S_{t-1,i}T_i -\sum T_iS_{t+2,i}\sum S_{t-1,i})\\
        & = &-\sum S_{t-1,i}^2((\sum{T_i})^2-n\sum{T_i^2})\sum S_{t-1,i}( \sum S_{t+2,i}\sum S_{t-1,i}T_i -\sum T_iS_{t+2,i}\sum S_{t-1,i}) \\ 
        & = &-\sum S_{t-1,i}^2\sum S_{t-1,i}((\sum{T_i})^2-n\sum{T_i^2})( \sum S_{t+2,i}\sum S_{t-1,i}T_i -\sum T_iS_{t+2,i}\sum S_{t-1,i})\\
        & = & \sum S_{t-1,i}^2\sum S_{t-1,i}(n\sum{T_i^2} - (\sum{T_i})^2)( \sum S_{t+2,i}\sum S_{t-1,i}T_i -\sum T_iS_{t+2,i}\sum S_{t-1,i})\\
        & = &\sum S_{t-1,i}^2\sum S_{t-1,i}(n\sum T_i^2\sum S_{t+2,i}\sum S_{t-1,i}T_i + (\sum{T_i})^2\sum T_iS_{t+2,i}\sum S_{t-1,i} \\
        & &  - (\sum{T_i})^2\sum S_{t+2,i}\sum S_{t-1,i}T_i - n\sum{T_i^2}\sum T_iS_{t+2,i}\sum S_{t-1,i})\\ 
        & = &\sum S_{t-1,i}^2\sum S_{t-1,i}(\sum T_i^2\sum S_{t+2,i}\sum S_{t-1,i}\sum T_i + n(\sum{T_i})\sum T_iS_{t+2,i}\sum T_iS_{t-1,i} \\
        & &  - (\sum{T_i})^2\sum S_{t+2,i}\sum S_{t-1,i}T_i - n\sum{T_i^2}\sum T_iS_{t+2,i}\sum S_{t-1,i})\\         
        &=&  \sum S_{t-1,i}^2\sum S_{t-1,i}( n\sum T_iS_{t+2,i}-\sum S_{t+2,i}\sum T_i)(\sum T_i\sum T_iS_{t-1,i}-
         \sum S_{t-1,i}\sum T_i^2)\\
        &=&  \sum S_{t-1,i}^2( n\sum T_iS_{t+2,i}-\sum S_{t+2,i}\sum T_i)\sum S_{t-1,i}(\sum T_i\sum T_iS_{t-1,i}-
        \sum S_{t-1,i}\sum T_i^2)\\
        & = & n\sum S_{t-1,i}^2\sum (T_i - \overline T)(S_{t+2,i} - \overline{S_{t+2}})\sum S_{t-1,i}(\sum T_i\sum T_iS_{t-1,i}-
        \sum S_{t-1,i}\sum T_i^2)
      \end{eqnarray*}
      Thus,
      $$
        \frac{\sum S_{t-1,i}( \sum S_{t+2,i}\sum S_{t-1,i}T_i -\sum T_iS_{t+2,i}\sum S_{t-1,i})}{n\sum S_{t-1,i}^2\sum (T_i - \overline T)(S_{t+2,i} - \overline{S_{t+2}})}= 
        \frac{\sum S_{t-1,i}(\sum T_i\sum T_iS_{t-1,i}-
        \sum S_{t-1,i}\sum T_i^2)}{n\sum S_{t-1,i}^2\sum (T_i - \overline T)^2}
      $$
  And so,
  \begin{eqnarray*}
    \hat \beta &=&\frac{ -n\sum S_{t-1,i}^2\sum (T_i - \overline T)(S_{t+2,i} - \overline{S_{t+2}}) 
        -\sum S_{t-1,i}( \sum S_{t+2,i}\sum S_{t-1,i}T_i -\sum T_iS_{t+2,i}\sum S_{t-1,i})}
        {-n\sum S_{t-1,i}^2\sum (T_i - \overline T)^2  - \sum S_{t-1,i}(\sum T_i\sum T_iS_{t-1,i}
        - \sum S_{t-1,i}\sum T_i^2)}\\
        &=&\frac{  (-n\sum S_{t-1,i}^2\sum (T_i - \overline T)(S_{t+2,i} - \overline{S_{t+2}}))(1+
          \frac{\sum S_{t-1,i}( \sum S_{t+2,i}\sum S_{t-1,i}T_i -\sum T_iS_{t+2,i}\sum S_{t-1,i})}{n\sum S_{t-1,i}^2\sum (T_i - \overline T)(S_{t+2,i} - \overline{S_{t+2}}))})}
          {(-n\sum S_{t-1,i}^2\sum (T_i - \overline T)^2)(1+\frac{\sum S_{t-1,i}(\sum T_i\sum T_iS_{t-1,i}
        - \sum S_{t-1,i}\sum T_i^2)}{n\sum S_{t-1,i}^2\sum (T_i - \overline T)^2}}\\
        &=&\frac{-n\sum S_{t-1,i}^2\sum (T_i - \overline T)(S_{t+2,i} - \overline{S_{t+2}})}{-n\sum S_{t-1,i}^2\sum (T_i - \overline T)^2}   = \frac{\sum (T_i - \overline T)(S_{t+2,i} - \overline{S_{t+2}})}{\sum (T_i - \overline T)^2}
  \end{eqnarray*}
  That is, the $\hat \beta$ in this model is the same as the $\hat \beta$ in the model without $S_{t-1}$. 
  Thus, $\hat \beta$ is an unbiased estimate of $\beta$.
  \item[b)]
    The process for part a) can be followed for part b).  
    However, the correlation between $T_i$ and $S_{t+1,i}$ is not zero, 
    and in fact, is probably positive.
    Similar to 4d), the result will then be biased.
  \item[c)] The estimate will be biased asymptotically.
    The non-zero correlation between $T_i$ and $S_{t+1,i}$ will exist asymptotically,
    and so, the estimate in expectation will be biased.
    \end{itemize}
\end{itemize}

 \newpage
\begin{verbatim}
#####################
#  PS C236A / STAT C239A 
#
#  John Henderson
#   HW 1 - Solutions: 6 - 9
#
# Sept. 29, 2012      
#####################
    
rm(list=ls())
# 6. Potential Outcomes from an Experiment 

yt=c(2,6,33,17,2,54)
yc=c(1,2,13,14,10,3) 
     
# (a)  unit-level and average effects 
tau_i=yt-yc     
print(tau_i)
#1  4 20  3 -8 51

tau_bar=mean(tau_i)
#11.833

# The ATE is reasonable, although a median treatment effect is helpful
#  since there are two large outliers, and the response surface is 
#  thus skewed.

# (b) variance 
var_yt=sum((yt-mean(yt))^2)/6
var_yc=sum((yc-mean(yc))^2)/6
cov_y=sum((yt-mean(yt))*(yc-mean(yc)))/6  

var.usual=6/(6-1)*(var_yt/3 + var_yc/3)
var.true=6/(6-1)*(var_yt/3 + var_yc/3) + (1/(6-1))*(2*cov_y - var_yt - var_yc)      

#Note that the true variance is smaller than the "usual" variance


# (c)

exp.fun=function(y_t=yt, y_c=yc, n.treat){
  #create a treatment vector
   treat.assign = matrix(0,ncol=1,nrow=length(y_t))
   #Randomly sample and assign treatment
   treat.assign[sample(1:length(treat.assign),n.treat)] = 1
   #produce observed values
   y_t.obs = y_t[treat.assign==1]
   y_c.obs = y_c[treat.assign==0]
   #calculate estimate average treatement effect
   ate = mean(y_t.obs) - mean(y_c.obs)
   #calculate the standard error
   ate.se = sqrt(var(y_t.obs)/length(y_t.obs) + var(y_c.obs)/length(y_c.obs))
   #create a list with our desired outputs
   output = list(y_t.obs = y_t.obs, y_c.obs=y_c.obs, ate = ate, ate.se = ate.se)
   #return the output
   return(output)
}

#Lets make sure the function works
set.seed(1005)
exp.fun(yt,yc,3)


# (d)
##calculate every possible permutation
perms = combn(6,3)

#create a matrix to hold all the average treatment effectx
ates = matrix()

##loop over every combination and calculate the treatment effect
for (i in 1:ncol(perms)){
	ates[i] =  mean(yt[perms[,i]]) - mean(yc[-1* perms[,i]]) 
}

#plot a histogram showing the distribution of the treatment effects
hist(ates,breaks=8, col="darkgreen")

#Calculate the variance
var(ates)                   
# 85.433

# where true variance is 86.9 
         

# 7 Olken Data and ATT with OLS

#load library for making tables
library(xtable)
rm(list=ls(all=TRUE))

#load data
load(file=url("http://sekhon.berkeley.edu/causalinf/data/hw1data.RData"))

# (a) Average differences 
     
y = data$pct.missing    
tr = data$treat.invite

tau_bar=mean(y[tr==1],na.rm=T)-mean(y[tr==0],na.rm=T)

#for Standard error,  you need number of units in control,  number in treatment,
#sample variance of treated units,  sample variance of control units
n.treat = sum(tr)
n.control = sum(1 - tr)      

var.treated = var(y[tr == 1], na.rm = TRUE) / n.treat
var.control = var(y[tr == 0], na.rm = TRUE) / n.control
itt.se = sqrt(var.treated + var.control)  
               

# lets write a function to collect this information together 
# mean.dif
mean.dif=function(y,tr){
	tau_bar=mean(y[tr==1],na.rm=T)-mean(y[tr==0],na.rm=T)
	
	n.treat = sum(tr)
	n.control = sum(1 - tr)      

	var.treated = var(y[tr == 1], na.rm = TRUE) / n.treat
	var.control = var(y[tr == 0], na.rm = TRUE) / n.control
	itt.se = sqrt(var.treated + var.control)	   
	      
	return(list("mean.dif"=tau_bar,"se"=itt.se)) 
}


# (b)  write an OLS function using matrix computations
    
ols.matrix=function(Y,X){
	         		
	if(any(is.na(Y))){
		stop("Missing values on the outcome variable, OLS estimates not identified.")
	}               
	
	if(any(is.na(X))){
		stop("Missing values on a covariate, OLS estimates not identified.")
	} 
	
	n=length(Y)
	p=dim(X)[2]

	# OLS estiamtes
	beta.ols=solve(t(X)%*%(X))%*%(t(X)%*%(Y))     
                   
	# Projection matrix for SE estimates
	h=X%*%solve(t(X)%*%X)%*%t(X)
	i=(h-h)
	diag(i)=1  
	hhat=i-h               
	e=hhat%*%Y
     	
	sigma2=sum(e^2)/(n-p)

	varsB=sigma2*solve(t(X)%*%X)
	se.ols=t(t(sqrt(diag(varsB))))
	                    
	st=dim(X)[2]
	    
	var.names=c()       
	for(i in 2:st){
		var.names=c(var.names,paste('v',i-1,sep=''))
	} #END var.name loop     
	            
	var.names=c('alpha',var.names)   
	outs=matrix(NA,length(beta.ols),2)        
	outs[,1]=beta.ols            
	outs[,2]=se.ols
	rownames(beta.ols)=rownames(se.ols)=rownames(outs)=var.names
	colnames(outs)=c('beta','se')
	  
	return(outs)       	  
	
} # END ols.matrix function
                           

X=cbind(1,tr)
Y=y
X=X[!is.na(Y),]
Y=Y[!is.na(Y)]   

ols.matrix(Y,X)

 
# (c)
                                                                                                             
misses=array(TRUE,length(y))
X=cbind(1,tr,data$share.total.unskilled,data$head.edu,data$mosques,
  data$pct.poor,data$total.budget)          
Y=y          
mMat=cbind(Y,X)
for(j in 1:ncol(mMat)){
	inds=which(is.na(mMat[,j])) 
	if(length(inds>0)){
		misses[inds]=FALSE 
	}	
}
Y=Y[misses]
X=X[misses,]

ols.matrix(Y,X)   

# Note the 'data$unskilled.transformed' should be dropped due to 
  collinearity with 'data$share.total.unskilled'


# 8. ATT via OLS regression        

# use ols.matrix from above 
      
misses=array(TRUE,length(y))
X=cbind(1,tr,data$mosques,data$unskilled.transformed)          
Y=y          
mMat=cbind(Y,X)
for(j in 1:ncol(mMat)){
	inds=which(is.na(mMat[,j])) 
	if(length(inds>0)){
		misses[inds]=FALSE 
	}	
}
Y=Y[misses]
X=X[misses,]
      

# consider OLS fixed, and estimate the full model 

# (a)        

Tr=tr[misses]    
   
betas=ols.matrix(Y,X)[,1]     

Xtr=X[Tr==1,]
Xct=X[Tr==1,]   
Xct[,2]=0
            

X_rep=rbind(Xtr,Xct)
Y_pred=X_rep%*%betas
           
mean(Y_pred[X_rep[,2]==1]-Y_pred[X_rep[,2]==0])
ols.matrix(Y_pred,X_rep)[2,1]
            

# (b) interactive model

misses=array(TRUE,length(y))
X=cbind(1,tr,data$unskilled.transformed,data$mosques,
  tr*data$unskilled.transformed)          
Y=y          
mMat=cbind(Y,X)
for(j in 1:ncol(mMat)){
	inds=which(is.na(mMat[,j])) 
	if(length(inds>0)){
		misses[inds]=FALSE 
	}	
}
Y=Y[misses]
X=X[misses,]
   
betas=ols.matrix(Y,X)[,1]     
           
Xtr=X[Tr==1,]
Xct=X[Tr==1,]   
Xct[,5]=Xct[,2]=0
            
X_rep=rbind(Xtr,Xct)  
Y_pred=X_rep%*%betas

#Y_pred_1=Xtr%*%betas
#Y_pred_0=Xct%*%betas
                
#mean(Y_pred_1-Y_pred_0)   
mean(Y_pred[X_rep[,2]==1]-Y_pred[X_rep[,2]==0])
            


# END HW1_Answers.R
           

\end{verbatim}

\end{document}
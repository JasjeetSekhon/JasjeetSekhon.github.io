\documentclass{article}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{color}
\usepackage{setspace}
\usepackage{fullpage}
\usepackage[round]{natbib}
\usepackage[utf8]{inputenc}
 
\usepackage{fullpage}
\usepackage{boxedminipage}

\usepackage{listings}

\usepackage{minitoc}

\usepackage{ifpdf}
\usepackage{amsmath}
\usepackage{latexsym}
\usepackage{amssymb}

\usepackage{natbib} 
\usepackage{times} 
\usepackage{setspace}
\usepackage{subfigure}

\usepackage{hyperref} 
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}} 
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}} 
\newcommand{\var}[0]{\text{var}}
\newcommand{\cov}[0]{\text{cov}}
\def\E{{\mathbb E}}   
\ifpdf 
\usepackage[pdftex]{graphicx} \else 
\usepackage{graphicx} \fi 

\newcommand{\indep}{\perp\!\!\!\perp}

\begin{document}

%\flushright
%Mike Higgins\\
%Stat 239A\\
%Midterm\\
%\flushleft
\title{PS C236A / Stat C239A \\ Problem Set 6 - Solutions}
\date{}
\maketitle
\begin{itemize}

\item[{\large \bf I}] {\large \bf True or False}
\begin{itemize}
\item[1] {\bf False} -- These are two estimators of the same quantity,
  $E(Y_1)-E(Y_0)$, which by construction are identical. So OLS here
  tells us nothing about whether $E(Y_1)-E(Y_0)$ is an unbiased
  estimator of $E(Y_1-Y_0)$ given the design.  Moreover, for the
  randomization to have worked well, this means that $Y \indep T$,
  which emerges from whether or not the design was well-implemented
  and treatment was truly randomized.
\item[2] {\bf False} -- A $t$-test is testing the null hypothesis of
  no difference in two-sample averages, regardless of whether or not
  individual draws are zero or non-zero; A permutation inference tests
  a sharp null hypothesis that each unit exhibits precisely no
  difference when assigned to a treatment or control population: this
  test is exact without requiring large sample approximations


\item[3] {\bf False} -- The effect the researchers are estimating (the
  total average of pill takers minus the total average of placebo
  takers) is an outome that is the combination of two treatments
  $T_a$ and $T_b$, where $T_b$ potentially influences compliance rates
  for $T_a$.  Define the potential outcomes of the two-stage
  experiment to be: 
\begin{eqnarray}
\nonumber Y_{ab}& = &Y_{11}\ \text{if}\ T_a=1\ \text{and}\ T_b=1 \\ 
\nonumber& = &Y_{10}\ \text{if}\ T_a=1\ \text{and}\ T_b=0 \\ 
\nonumber& = &Y_{01}\ \text{if}\ T_a=0\ \text{and}\ T_b=1 \\ 
\nonumber& = &Y_{00}\ \text{if}\ T_a=0\ \text{and}\ T_b=0 
\end{eqnarray}
The researchers estimate:
$\hat{\delta}=E\{Y_{11},Y_{10}\}-E\{Y_{01},Y_{00}\}$, and assert that
$\hat{\delta}$ is an unbiased estimate of $\gamma=E\{Y_1-Y_0\}$, where there
is some rate of non-compliance for $T_a$ at $\alpha$. 
\vspace{1em}

It is sufficient to show that this $\hat{\delta}$ can be a biased
estimate of $\gamma$ under general conditions, that is without
additional assumptions.  One such condition is when
$\alpha_{11}>\alpha_{10}$ for treated units randomly assigned to
the $t+1$ encouragement treatment or control conditions, and thus if
the encourgement condition works to lower non-compliance on $T_a$. To
see this, we know that
 $$E\{Y_{10}\}-E\{Y_{00}\}=E\{Y_{1}\}-E\{Y_{0}\}$$
and if $\alpha_{11}>\alpha_{10}$, then:
$$E\{Y_{11}\}-E\{Y_{01}\}>E\{Y_{10}\}-E\{Y_{00}\}$$
which implies that
$$E\{Y_{11}-c\}-E\{Y_{01}\}=E\{Y_{10}\}-E\{Y_{00}\}$$
$$E\{Y_{11}-c\}-E\{Y_{01}\}=E\{Y_{1}\}-E\{Y_{0}\}$$
By property of expectation:
$$E\{Y_{11}-c,Y_{10}\}-E\{Y_{01},Y_{00}\}=E\{Y_{1}\}-E\{Y_{0}\}$$
For $c\neq0$, $E\{Y_{11}-c,Y_{10}\}\neq E\{Y_{11},Y_{10}\}$, so $\hat{\delta}\neq\gamma$.


\end{itemize}
\vspace{1em}

\item[{\large \bf  II}] {\large \bf  Sample Selection}
\begin{itemize}
\item[a.] 	Let $p_{j,k}$ denote the proportion of candidates with $D_i = j$ and $R_i = k$.
	We can write
	\begin{eqnarray}
	&&\E(Y_i(1) - Y_i(0))\nonumber \\
	&=&\E(Y_i(1) - Y_i(0)| D_i = 1, R_i = 1)p_{1,1} +
	   \E(Y_i(1) - Y_i(0)| D_i = 1, R_i = 0)p_{1,0}\nonumber \\
	&+&\E(Y_i(1) - Y_i(0)| D_i = 0, R_i = 1)p_{0,1}+
		 \E(Y_i(1) - Y_i(0)| D_i = 0, R_i = 0)p_{0,0}.
	\label{condexps}
	\end{eqnarray}
	It follows from \eqref{condexps} that
	\begin{align}
		\E(\hat \delta_{naive})  = & \E(Y_i(1)|D_i = 1, R_i = 1) - \E(Y_i(0)|D_i = 0, R_i = 1) 
		\nonumber \\
		= &\E(Y_i(1) - Y_i(0)) -\E(Y_i(1) - Y_i(0)| D_i = 1, R_i = 0)p_{1,0} \nonumber \\
		 &- \E(Y_i(1) - Y_i(0)| D_i = 0, R_i = 0)p_{0,0} + \E(Y_i(0)| D_i = 1, R_i = 1)p_{1,1} 
		 \nonumber \\
		 & - \E(Y_i(1)| D_i = 0, R_i = 1)p_{0,1} + \E(Y_i(1)| D_i = 1, R_i = 1)(1 - p_{1,1}) \nonumber \\
		 & - \E(Y_i(0)|D_i = 0, R_i = 1)(1 - p_{0,1}).
		 \label{ugequal}
	\end{align}
	Thus, the bias in the estimator is
	\begin{align}
		Bias(\hat \delta_{naive}) = &
		 - \E(Y_i(1) - Y_i(0)| D_i = 1, R_i = 0)p_{1,0} -\E(Y_i(1) - Y_i(0)| D_i = 0, R_i = 0)p_{0,0} \nonumber \\
		 & + \E(Y_i(0)| D_i = 1, R_i = 1)p_{1,1}  - \E(Y_i(1)| D_i = 0, R_i = 1)p_{0,1} \nonumber \\
		 & + \E(Y_i(1)| D_i = 1, R_i = 1)(1 - p_{1,1}) - \E(Y_i(0)|D_i = 0, R_i = 1)(1 - p_{0,1}). 
		 \label{bias}
	\end{align}
\item[b.] 
	If $(Y_i(1),Y_i(0)) \indep D_i$, then \eqref{bias} simplifies to
	\begin{align}
		& - \E(Y_i(1) - Y_i(0)| R_i = 0)p_{1,0} - \E(Y_i(1) - Y_i(0)| R_i = 0)p_{0,0} \nonumber \\
		 & + \E(Y_i(0)|  R_i = 1)p_{1,1}  - \E(Y_i(1)|  R_i = 1)p_{0,1} \nonumber \\
		 & + \E(Y_i(1)|  R_i = 1)(1 - p_{1,1}) - \E(Y_i(0)| R_i = 1)(1 - p_{0,1}) \nonumber \\
		 = &- \E(Y_i(1) - Y_i(0)| R_i = 0)(p_{1,0} + p_{0,0}) \nonumber \\
		 & + \E(Y_i(1)| R_i = 1)(1 - p_{1,1} - p_{0,1}) \nonumber \\
		 & - \E(Y_i(0)|R_i = 1)(1 - p_{1,1} - p_{0,1}).
		 \label{simplertime}
	\end{align}
	Note that $p_{1,1} + p_{0,1} = \lambda$ and $p_{1,0} + p_{0,0} = 1 - \lambda$.
	Thus, we can write the bias in \eqref{simplertime} as
	\begin{equation}
		 Bias(\hat \delta_{naive}) = \left [\E(Y_i(1) - Y_i(0)| R_i = 1)- 
		 \E(Y_i(1) - Y_i(0)| R_i = 0)\right ](1-\lambda).
		 \label{dabias}
	\end{equation}
	The bias is zero if and only if
	the incumbency advantage among candidates
	that run in the next election is the same 
	as the advantage for those that do not run in the next election.
\item[c.]
	If the incumbency advantage is at least as big among candidates that re-run,
	and if the independence assumption holds,
	then $Bias(\hat \delta_{naive}) \geq 0$.  
	That is, the smallest possible value of $\E(\delta)$ is $\E(\hat \delta_{naive})$.
	
	Since $0 \leq Y_i(j) \leq 1$, it follows that $-1 \leq \E(Y_i(1) - Y_i(0)|R_i) \leq 1$.  
	Thus, if the incumbency advantage is at least as big among candidates that re-run,
	it follows that $Bias(\hat \delta_{naive}) \leq 2(1-\lambda).$
	(If most of the candidates run in the next election, this bias is small).
	
	More precisely, under the assumption that $(Y_i(1),Y_i(0)) \indep D_i$,
	it follows that $\E(\hat \delta_{naive}) = \E(Y_i(1) - Y_i(0)| R_i = 1)$.
	Thus, 
	\begin{eqnarray}
	  Bias(\hat \delta_{naive}) &=& \E(\hat \delta_{naive}) - \E(\delta) \nonumber\\
	  &=&\left [\E(\hat \delta_{naive})- \E(Y_i(1) - Y_i(0)| R_i = 0)\right ](1-\lambda)	
	\end{eqnarray}
	And so,
	\begin{eqnarray}
	  \E(\delta) = \lambda\E(\hat \delta_{naive}) + 
	  (1-\lambda)\E(Y_i(1) - Y_i(0)| R_i = 0) \leq \lambda\E(\hat \delta_{naive}) + (1-\lambda)
	\end{eqnarray}
\end{itemize}

\item[{\large \bf  III}] {\large \bf  Regression Discontinuity}
\begin{itemize}
\item[a.] 
	In this scenario, we have a 
	binomial-randomized controlled experiment ($p$ = 0.5);
	by design, we are guaranteed that treatment assignment is 
	independent of potential outcome.
	
	A natural quantity to estimate given this design is the ATE of
	the training program on status and income.  
	We can make inferences about the ATE over all workers
	in our study.
	
	Suppose there are $N$ workers in total.
	Let $T_i = 1$ if worker $i$ participates in the 
	training program, and $T_i = 0$ if the worker does not.
	Let $Y_i(1)$ and $Y_i(0)$ denote the 
	value of the response variable for worker $i$
	(either status or income) under treatment
	and control respectively.
	Estimates of the ATE include:
	\begin{equation}
		\sum_{i = 1}^N \frac{Y_i(1)T_i}{\sum T_i} - \sum_{i = 1}^N \frac{Y_i(0)(1-T_i)}{\sum(1-T_i)}.
	\end{equation}
	and
	\begin{equation}
		\frac 1 {N/2}  \sum_{i = 1}^N Y_i(1)T_i - \frac 1 {N/2} \sum_{i = 1}^N Y_i(0)(1-T_i).
	\end{equation}
	Both estimates can be shown to be unbiased.
%	We can estimate this unbiasedly using
%	$$
%		\frac{1}{\sum T_i}\sum_{T_i = 1} Y_i - \frac{1}{\sum( 1-T_i)}\sum_{T_i = 0} Y_i.
%	$$
	Inferences assume the SUTVA assumption.
	Through this assumption,
	we can test the (sharp) null hypothesis of no treatment effect
	on all workers using a permutation test.
%	Moreover, if we make further assumptions on how
%	the treatments may effect workers,
%	(say, a constant additive or multiplicative treatment effect),
%	we can invert this hypothesis test and obtain confidence
%	intervals for the ATE.
\item[b.]	
	Despite randomization into treatment and control, 
	we should not use the approach as in part a);
	workers may respond differently depending on the
	amount of compensation they receive before beginning the program.
	
	Instead, this may be a more reasonable assumption: 
	the compensation for units that were assigned a number $X \approx 2$
	is similar enough that the effect of treatment on units that had $X$ slightly
	greater than 2 would be the same as the effect of treatment with 
	for units with $X$ slightly less than 2 had they been assigned treatment.
	Because assignment to treatment is determined by a fixed threshold of $X$, 
	this suggests making inference using a (sharp) regression discontinuity design.
	We would estimate the local average treatment effect (LATE) at $X = 2$;
	$$
		\E(Y_i(1) - Y_i(0) | X_i = 2).
	$$

	We can obtain an estimate of the local average
	treatment effect (LATE) $\hat \beta$ at $X=2$ 
	by fitting two OLS models (or LOESS, or whatever your favorite method is)
         over data within some small bandwidth $\delta$ around $X = 2$;
         one model fitted to data before the cutpoint and one fitted to data after the cutpoint.
	The estimate of the LATE is the difference of the model estimates at the cutpoint.
\item[c.]
	Let $Z_i = 1$ if the coin flips heads and let $Z_i = 0$ if the coin flips tails.
	Let $P(Z_i = 1) = p$
	Recall, the LATE is
	$$
	  \E(Y_i(1) - Y_i(0) | X_i = 2)
	$$
	What we estimate to the left of the cutpoint is:
	$$
	  \E(Y_i(0)|X_i = 2)
	$$
	and what we estimate to the right of the cutpoint is
	$$
	  p\E(Y_i(1) | X_i = 2, Z_i = 1) + (1-p)\E(Y_i(0) | X_i = 2, Z_i = 0)
	$$
	If smoothness conditions necessary for RD hold,
	they will still hold in this model, as long as response is independent of the coin flip
	(if people can self-select to be in treatment or control, this property will no longer hold).
	The quantity estimated using RD will then be
	$$
	  p\E(Y_i(1) | X_i = 2) + (1-p)\E(Y_i(0) | X_i = 2) - \E(Y_i(0)|X_i = 2) =
	  p(\E(Y_i(1) | X_i = 2) - \E(Y_i(0) | X_i = 2))
	$$
	This expectation scales with the probability of heads of the coin flip.
	If $p$ is known, an estimate of the LATE can be obtained by 
	estimating the LATE without factoring in the coin flip, and then dividing by $1/p$.
\end{itemize}
\item[{\large \bf IV}] {\large \bf Media Bias}
\\

See \texttt{hw5\_solutions.pdf} for an example.
\\

\item[{\large \bf V}] {\large \bf Data and Matching} 
\\

See \texttt{HW6\_Answers.R} for solutions
\end{itemize}
\end{document}